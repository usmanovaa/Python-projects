{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e15ccb",
   "metadata": {},
   "source": [
    "# Job Data Web Scraping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Problem statement:\n",
    "\n",
    "We want to get a dataset of all job postings from an Indian job search resource \"https://www.naukri.com/\". \n",
    "\n",
    "#### Input:\n",
    "\n",
    "We have a file 'link_by_areas.csv' with the links to the job postings by area.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "CSV file which contains job title, name of company, expected experience, salary, tags associsted, function area, posting date, scraping date.\n",
    "\n",
    "Data should be scraped for every link, looping through all the job postings page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effcec98",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/jobs_site.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb9ded",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "    \n",
    "   1. Importing libraries, reading input data\n",
    "   2. Understanding the URL structure\n",
    "   3. Scraping data from Naukri.com\n",
    "   4. Saving the scraped data into a csv file\n",
    "\n",
    "#### 1. Importing libraries, reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9912ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f988d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblinks = pd.read_csv('data/link_by_areas.csv')\n",
    "print('Number of job areas:', len(joblinks))\n",
    "joblinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24808c",
   "metadata": {},
   "source": [
    "#### 2. Understanding the URL structure\n",
    "\n",
    "\n",
    "We want to creat a general rule to scrape from any link from our list.\n",
    "\n",
    "- All the links have a domain: \"https://www.naukri.com/\".\n",
    "\n",
    "- Then what job we search: \"interior-deesign\".\n",
    "\n",
    "- Then \"-jobs\".\n",
    "\n",
    "- Then number of a page \"-2\" (just not for the 1st page). <---- **We need to add it to our list of links.**\n",
    "\n",
    "- Then its query tail \"?xt=catsrch&qf[]=12\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = joblinks['link'].tolist()\n",
    "urls[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ccbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yarl\n",
    "from yarl import URL # library to take parts from the link\n",
    "\n",
    "jobs_part = []\n",
    "\n",
    "for i in urls:\n",
    "    jobs_part.append(URL(i).path)\n",
    "    \n",
    "jobs_part[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "tails = []\n",
    "\n",
    "for i in urls:\n",
    "    tails.append(URL(i).query_string)\n",
    "tails[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d91280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating geniral link to use it then for different pages\n",
    "\n",
    "gen_urls = []\n",
    "\n",
    "for i in range(len(tails)):\n",
    "    \n",
    "    url = 'https://www.naukri.com' + jobs_part[i] + '{}?' + tails[i]\n",
    "    gen_urls.append(url)\n",
    "    \n",
    "gen_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f62b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(joblinks['link']) == len(gen_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058cdfb1",
   "metadata": {},
   "source": [
    "#### 3. Scraping data from Naukri.com\n",
    "\n",
    "- Defining a DataFrame\n",
    "- Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb06d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity of computation we observe in the code only first page and \n",
    "# first link from the input file\n",
    "\n",
    "df_posts = pd.DataFrame(columns = ['Function_Area', 'Job_Title', 'Experience', 'Company', 'Scraping_Date', 'Salary', 'Location', 'Tags_Associated', 'Posting_Date'])\n",
    "    \n",
    "for urll in gen_urls[2:4]:\n",
    "    \n",
    "    for page in range(1,3):\n",
    "        \n",
    "        \n",
    "        if page == 1:\n",
    "            str_page = ''\n",
    "        else:\n",
    "            str_page = '-'+str(page)\n",
    "            \n",
    "        url = urll.format(str_page)  # adding page to the link\n",
    "        \n",
    "        #print(url)\n",
    "        \n",
    "        driver = webdriver.Chrome('/Users/anastasia/Desktop/global-warming/chromedriver') # start the browser\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        time.sleep(15) # not to be blocked as a bot\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html')\n",
    "        \n",
    "        \n",
    "        driver.close()\n",
    "        \n",
    "        #print(soup.prettify())\n",
    "        \n",
    "        \n",
    "        results = soup.find('div', class_ = 'list')\n",
    "        job_elems = results.find_all('article', class_ ='jobTuple bgWhite br4 mb-8')\n",
    "        \n",
    "        \n",
    "        #print(len(job_elems))\n",
    "        \n",
    "        for job_elem in job_elems:\n",
    "            \n",
    "\n",
    "            # Job_Title\n",
    "            Job_Title = job_elem.find('a', class_ = 'title fw500 ellipsis')\n",
    "        \n",
    "        \n",
    "            # Experience\n",
    "            Exp = job_elem.find('li', class_ = 'fleft grey-text br2 placeHolderLi experience')\n",
    "            \n",
    "            if Exp is None:\n",
    "                Experience = None\n",
    "                continue\n",
    "            else:\n",
    "                exp_span = Exp.find('span', class_ = 'ellipsis fleft fs12 lh16')\n",
    "                if exp_span is None:\n",
    "                    Experience = None\n",
    "                    continue\n",
    "                else:\n",
    "                    Experience = exp_span.text\n",
    "                \n",
    "                \n",
    "            # Company\n",
    "            Company = job_elem.find('a', class_ = 'subTitle ellipsis fleft')\n",
    "            \n",
    "            \n",
    "            # Date Scraped\n",
    "            from datetime import date\n",
    "            today = date.today()\n",
    "            date_today = today.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "            \n",
    "            # Salary\n",
    "            Sal = job_elem.find('li', class_ = 'fleft grey-text br2 placeHolderLi salary')\n",
    "            if Sal is None:\n",
    "                Salary = None\n",
    "                continue\n",
    "            else: \n",
    "                Sal_span = Sal.find('span', class_ = 'ellipsis fleft fs12 lh16')\n",
    "                if Sal_span is None:\n",
    "                    Salary = None\n",
    "                    continue\n",
    "                else:\n",
    "                    Salary = Sal_span.text\n",
    "\n",
    "                    \n",
    "            # Location for the job post\n",
    "            Loc = job_elem.find('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "            if Loc is None:\n",
    "                Location = None\n",
    "                continue\n",
    "            else:\n",
    "                Loc_span = Loc.find('span', class_ = 'ellipsis fleft fs12 lh16')\n",
    "                if Loc_span is None:\n",
    "                    Location = None\n",
    "                    continue\n",
    "                else:\n",
    "                    Location = Loc_span.text\n",
    "\n",
    "                    \n",
    "            # Tags\n",
    "            all_tags = job_elem.find_all('li', class_ = 'fleft fs12 grey-text lh16 dot')\n",
    "            \n",
    "            if all_tags is None:\n",
    "                assoc_tags = None\n",
    "                continue\n",
    "            else:\n",
    "                assoc_tags = []\n",
    "                for tag in all_tags:\n",
    "                    assoc_tags.append(tag.text)\n",
    "\n",
    "                \n",
    "            # Date job was posted\n",
    "            \n",
    "            date = job_elem.find('div', class_ = 'type br2 fleft green')\n",
    "            if date is None:\n",
    "                continue\n",
    "            else:\n",
    "                date_span = date.find('span', class_ = 'fleft fw500')\n",
    "                if date_span is None:\n",
    "                    Posting_Date = None\n",
    "                else:\n",
    "                    Posting_Date = date_span.text\n",
    "                    \n",
    "            df_posts.loc[len(df_posts.index)] = [URL(url).path[1:], Job_Title.text, Experience, Company.text,\n",
    "                                                date_today, Salary, Location, assoc_tags, Posting_Date  ]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1dcd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5dbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837600c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = pd.DataFrame(columns = ['Function_Area', 'Job_Title', 'Experience', 'Company', 'Scraping_Date', 'Salary', 'Location', 'Tags_Associated', 'Posting_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ee716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fb1b0",
   "metadata": {},
   "source": [
    "#### 4. Saving the scraped data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5a827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts.to_csv('scrapped_job_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
